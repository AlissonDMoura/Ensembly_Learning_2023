{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056a857a",
   "metadata": {},
   "source": [
    "### Framework testing Introduction.\n",
    "\n",
    "**Welcome to the Framework introduction**\n",
    "In this Notebook we will follow a few steps to create a testing environment for an approach for image recognition called ensembly learning, which we will be using to filter data from a big dataset into a smaller known possible individuals.\n",
    "\n",
    "To initialyse this Framework, please be sure you have the Framework folder and its contents:\n",
    "* framework validation folder.\n",
    "* Models Folder.\n",
    "* Full Classes model folder.\n",
    "\n",
    "#### Framework validation\n",
    "This is a Folder containing one picture to represent all different individuals, there are 2000 different picture as a sample of each class. These pictures were not trained with and will be used as our validation of the models and this framework.\n",
    "\n",
    "#### Models Folder.\n",
    "This Folder contains the trained models for the classes we will be working with, there are a total of 100 models trained with the same parameters for 10 epochs and also a dictionary of dictionaries that will be used to label each model trained to a class name.\n",
    "\n",
    "#### Full Class Model Folder\n",
    "This FOlder contains a single model trained for all 2000 classes with 100 epochs, this model will be used as comparation of results with our Ensembly learning approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef89dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9961a96",
   "metadata": {},
   "source": [
    "#### Cell 1\n",
    "\n",
    "We start by loading a few libraries that we will be using initially, Not loading all at once here, so we can follow a though process and avoid confusion of the concept for the creation of this idea.\n",
    "\n",
    "* os - Will be used to operate with our operating system path mostly, so all paths will be standardized properly in each computer loading this notebook\n",
    "\n",
    "* tkinter - Will be used to create dialog windows for the user to be able to load the framework in their machine, this will setup the relative paths to all files we will be using in this demonstration.\n",
    "\n",
    "* numpy - Will be used to deal with out dictionaries and work with images as arrays for further processing.\n",
    "\n",
    "##### Please note that the dialog box might not show on top some times depending on your operational system, you will be able to find it using \"alt + tab\" or any similar way to browse trought applications running in your operational system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11671c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDIVIDUAL LOADED is 222\n",
      "MODELS FOLDER LOADED IN  C:\\Users\\aliss\\OneDrive\\Área de Trabalho\\last Semester\\Industry Project\\dataset\\Digiface\\FRAMEWORK\\Models\n",
      "Dictionary name updated as C:\\Users\\aliss\\OneDrive\\Área de Trabalho\\last Semester\\Industry Project\\dataset\\Digiface\\FRAMEWORK\\Models\\labels.npy\n"
     ]
    }
   ],
   "source": [
    "#Define the window\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "# Create a separate dialog window, place it on top of your screen and name the window as \"Select Models Folder\"\n",
    "dialog = tk.Toplevel(root)\n",
    "dialog.lift()\n",
    "dialog.attributes('-topmost', True)\n",
    "dialog.withdraw()\n",
    "#The window created will look for directories to be selected\n",
    "models_folder = filedialog.askdirectory(title=\"Select Models Folder\")\n",
    "\n",
    "# Create a separate dialog window, place it on top of your screen and will look for a file to be selected.\n",
    "dialog = tk.Toplevel(root)\n",
    "dialog.lift()\n",
    "dialog.attributes('-topmost', True)\n",
    "dialog.withdraw()\n",
    "image_path = filedialog.askopenfilename(parent=dialog)\n",
    "\n",
    "#normalyse the path as we'd have to use this for another relative path.\n",
    "models_folder = os.path.normpath(models_folder)\n",
    "#Save the image name for further comparison in the future\n",
    "image_name = os.path.basename(image_path)\n",
    "image_name = os.path.splitext(image_name)[0]\n",
    "#path for the dictionary file relative to the Models folder\n",
    "labels_file = os.path.join(models_folder, \"labels.npy\")\n",
    "\n",
    "# Display the results, showing the path for the Model folder, The name of the image and the path for the dictionary file.\n",
    "print(\"INDIVIDUAL LOADED is \" + image_name)\n",
    "print(\"MODELS FOLDER LOADED IN  \" + models_folder)\n",
    "print(\"Dictionary name updated as \"+ labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c32834b",
   "metadata": {},
   "source": [
    "### Cell 2\n",
    "\n",
    "After running this cell, we will have loaded in your system the paths for all we will be working with.\n",
    "Firstly a window will open for the user to select the Models folder, then to select one picture from our framework validation.\n",
    "\n",
    "It will save the paths relative to the image selected, the Model folder and also the Labels we need to identify the results in all models.\n",
    "\n",
    "It shows the output for the paths in your system for all these files and folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0068c89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 112, 112, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Resize the image to match the input size expected by the models\n",
    "image = Image.open(image_path)\n",
    "image = image.resize((112, 112))\n",
    "\n",
    "# Convert the image to RGB mode if it's not already\n",
    "if image.mode != \"RGB\":\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "# Convert the image to an array and normalize the pixel values\n",
    "image_array = np.array(image) / 255.0\n",
    "\n",
    "# Add an extra dimension to the image array to match the expected input shape of (None, 112, 112, 3)\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "print(image_array.shape)  # Verify the shape of the image array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96125e2",
   "metadata": {},
   "source": [
    "### Cell 2\n",
    "\n",
    "We will load a library to load the image\n",
    "\n",
    "* PIL - is responsible to load images and also perform some operations necessary for the image preprocessing.\n",
    "\n",
    "After running this cell, The image will be loaded in the system in a variable, preprocessed and also an array for the image will be create and also preprocessed in order to be in the shape expected by the models trained.\n",
    "\n",
    "The output will show the image array shape so we can see if the format, this is the format accepted by the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01c00ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Load the label dictionary\n",
    "label_dict = np.load(labels_file, allow_pickle=True).item()\n",
    "\n",
    "# Get the total number of keys in the dictionary\n",
    "total_keys = len(label_dict.keys())\n",
    "\n",
    "# Iterate over the keys\n",
    "for key in label_dict.keys():\n",
    "    value = label_dict[key]\n",
    "    \n",
    "print(total_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d1d31",
   "metadata": {},
   "source": [
    "### Cell 3\n",
    "\n",
    "The Labels dictionary is loaded from the path we had saved.\n",
    "to show its loaded properly, we display as an output the total of keys.\n",
    "we are expecting 100 dictionaries, one for each model trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39271b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Size of label_dict: 4696 bytes\n",
      "Disc Size of label_dict.npy: 33041 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Calculate the memory size of the label_dict (our dictionary of dictionaries.)\n",
    "label_dict_size = sys.getsizeof(label_dict)\n",
    "\n",
    "# Get the file size in bytes\n",
    "file_size = os.path.getsize(labels_file)\n",
    "\n",
    "# Display the size in memory\n",
    "print(\"Memory Size of label_dict:\", label_dict_size, \"bytes\")\n",
    "\n",
    "# Display the file size in disk\n",
    "print(\"Disc Size of label_dict.npy:\", file_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b074ae6",
   "metadata": {},
   "source": [
    "### Cell 4\n",
    "\n",
    "Here we show the sizes from the file in memory and in disk, This is to show the memory used for this method to map individuals is very interesting due to its memory use being low, even even the file size in disc would be much greater, its expecting to have a 4 digits memory size, even if the metadata in this file was weighting Gigabytes in disk.\n",
    "\n",
    "* sys - This library will use the operational system to perform the operatios of scal and get size in memory and in disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8adfc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32api\n",
    "from keras.models import load_model\n",
    "\n",
    "#load all files that are our models ('h5') and create a dictionary named models\n",
    "model_files = sorted([file for file in os.listdir(models_folder) if file.endswith(\".h5\")])\n",
    "models = {}\n",
    "\n",
    "#Iterate tought each file extracting file names, paths and the model metadata an saving in the dictionary.\n",
    "for model_file in model_files:\n",
    "    model_path = os.path.join(models_folder, model_file)\n",
    "    short_path = win32api.GetShortPathName(model_path)\n",
    "    model = load_model(short_path, compile=False)\n",
    "    model_name = os.path.splitext(model_file)[0]  # Extract the file name without the extension to be our key\n",
    "    models[model_name] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f16ff",
   "metadata": {},
   "source": [
    "### Cell 5\n",
    "\n",
    "With this cell we will load all our models in a dictionary that will hold all model name and its metadata.\n",
    "\n",
    "* wind32api - This library was important to create relative paths for loading and solve the problem with character encoding in some systems that could cause problems loading the models.\n",
    "* keras.models - this library will perform the reading of the files and save in a format we will be able to use with other keras library to perform our tasks further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68a6d244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "Model name: batch_1, Accuracy: 0.9956525564193726, Label: 1001\n",
      "Model name: batch_10, Accuracy: 0.9959709048271179, Label: 116\n",
      "Model name: batch_100, Accuracy: 0.6309670805931091, Label: 988\n",
      "Model name: batch_11, Accuracy: 0.9981791973114014, Label: 1180\n",
      "Model name: batch_12, Accuracy: 0.9987255930900574, Label: 1196\n",
      "Model name: batch_13, Accuracy: 0.7034862041473389, Label: 1230\n",
      "Model name: batch_14, Accuracy: 0.8203438520431519, Label: 1242\n",
      "Model name: batch_15, Accuracy: 0.9537598490715027, Label: 1262\n",
      "Model name: batch_16, Accuracy: 0.7583553791046143, Label: 128\n",
      "Model name: batch_17, Accuracy: 0.9359960556030273, Label: 1294\n",
      "Model name: batch_18, Accuracy: 0.5355360507965088, Label: 1314\n",
      "Model name: batch_19, Accuracy: 0.8710424304008484, Label: 1330\n",
      "Model name: batch_2, Accuracy: 0.9038170576095581, Label: 1015\n",
      "Model name: batch_20, Accuracy: 0.5058407187461853, Label: 1350\n",
      "Model name: batch_21, Accuracy: 0.8590993881225586, Label: 1374\n",
      "Model name: batch_22, Accuracy: 0.9875842928886414, Label: 1380\n",
      "Model name: batch_23, Accuracy: 0.961255669593811, Label: 1402\n",
      "Model name: batch_24, Accuracy: 0.9715534448623657, Label: 1422\n",
      "Model name: batch_25, Accuracy: 0.44804680347442627, Label: 1447\n",
      "Model name: batch_26, Accuracy: 0.4652256667613983, Label: 1459\n",
      "Model name: batch_27, Accuracy: 0.9577341079711914, Label: 1475\n",
      "Model name: batch_28, Accuracy: 0.7480084300041199, Label: 149\n",
      "Model name: batch_29, Accuracy: 0.911309540271759, Label: 1509\n",
      "Model name: batch_3, Accuracy: 0.9968026876449585, Label: 1048\n",
      "Model name: batch_30, Accuracy: 0.9006314873695374, Label: 1530\n",
      "Model name: batch_31, Accuracy: 0.8149033188819885, Label: 1539\n",
      "Model name: batch_32, Accuracy: 0.8659012317657471, Label: 1562\n",
      "Model name: batch_33, Accuracy: 0.914438009262085, Label: 1583\n",
      "Model name: batch_34, Accuracy: 0.9276613593101501, Label: 1605\n",
      "Model name: batch_35, Accuracy: 0.5019447207450867, Label: 1617\n",
      "Model name: batch_36, Accuracy: 0.44065406918525696, Label: 1630\n",
      "Model name: batch_37, Accuracy: 0.5543614029884338, Label: 1658\n",
      "Model name: batch_38, Accuracy: 0.5364143252372742, Label: 1681\n",
      "Model name: batch_39, Accuracy: 0.6400192975997925, Label: 1697\n",
      "Model name: batch_4, Accuracy: 0.9801499247550964, Label: 1053\n",
      "Model name: batch_40, Accuracy: 0.9464525580406189, Label: 171\n",
      "Model name: batch_41, Accuracy: 0.8208562731742859, Label: 172\n",
      "Model name: batch_42, Accuracy: 0.4961715638637543, Label: 1740\n",
      "Model name: batch_43, Accuracy: 0.808919370174408, Label: 1755\n",
      "Model name: batch_44, Accuracy: 0.919550359249115, Label: 1784\n",
      "Model name: batch_45, Accuracy: 0.8363339900970459, Label: 1792\n",
      "Model name: batch_46, Accuracy: 0.5995349884033203, Label: 181\n",
      "Model name: batch_47, Accuracy: 0.6715871095657349, Label: 1834\n",
      "Model name: batch_48, Accuracy: 0.8409633040428162, Label: 1858\n",
      "Model name: batch_49, Accuracy: 0.999925971031189, Label: 1879\n",
      "Model name: batch_5, Accuracy: 0.9392697811126709, Label: 107\n",
      "Model name: batch_50, Accuracy: 0.951323390007019, Label: 189\n",
      "Model name: batch_51, Accuracy: 0.9998741149902344, Label: 1900\n",
      "Model name: batch_52, Accuracy: 0.7800329923629761, Label: 1924\n",
      "Model name: batch_53, Accuracy: 0.9992198944091797, Label: 1947\n",
      "Model name: batch_54, Accuracy: 0.9724593162536621, Label: 1957\n",
      "Model name: batch_55, Accuracy: 0.9024055004119873, Label: 1971\n",
      "Model name: batch_56, Accuracy: 0.9959650039672852, Label: 200\n",
      "Model name: batch_57, Accuracy: 0.9563905000686646, Label: 206\n",
      "Model name: batch_58, Accuracy: 0.9896640181541443, Label: 229\n",
      "Model name: batch_59, Accuracy: 0.9207785129547119, Label: 258\n",
      "Model name: batch_6, Accuracy: 0.9962459206581116, Label: 1092\n",
      "Model name: batch_60, Accuracy: 0.9936979413032532, Label: 273\n",
      "Model name: batch_61, Accuracy: 0.9977037310600281, Label: 293\n",
      "Model name: batch_62, Accuracy: 0.9713810682296753, Label: 311\n",
      "Model name: batch_63, Accuracy: 0.9749558568000793, Label: 320\n",
      "Model name: batch_64, Accuracy: 0.9280320405960083, Label: 348\n",
      "Model name: batch_65, Accuracy: 0.8810470104217529, Label: 353\n",
      "Model name: batch_66, Accuracy: 0.4893723726272583, Label: 377\n",
      "Model name: batch_67, Accuracy: 0.7667683362960815, Label: 395\n",
      "Model name: batch_68, Accuracy: 0.4974977374076843, Label: 411\n",
      "Model name: batch_69, Accuracy: 0.8553231954574585, Label: 43\n",
      "Model name: batch_7, Accuracy: 0.5618774890899658, Label: 1121\n",
      "Model name: batch_70, Accuracy: 0.9265022873878479, Label: 457\n",
      "Model name: batch_71, Accuracy: 0.9378150105476379, Label: 475\n",
      "Model name: batch_72, Accuracy: 0.7160102725028992, Label: 485\n",
      "Model name: batch_73, Accuracy: 0.9999727010726929, Label: 508\n",
      "Model name: batch_74, Accuracy: 0.6774340867996216, Label: 527\n",
      "Model name: batch_75, Accuracy: 0.8381456136703491, Label: 536\n",
      "Model name: batch_76, Accuracy: 0.603377103805542, Label: 562\n",
      "Model name: batch_77, Accuracy: 0.9038387537002563, Label: 574\n",
      "Model name: batch_78, Accuracy: 0.9990047812461853, Label: 597\n",
      "Model name: batch_79, Accuracy: 0.9999144077301025, Label: 613\n",
      "Model name: batch_8, Accuracy: 0.8280950784683228, Label: 1123\n",
      "Model name: batch_80, Accuracy: 0.6333127021789551, Label: 63\n",
      "Model name: batch_81, Accuracy: 0.7721129655838013, Label: 654\n",
      "Model name: batch_82, Accuracy: 0.531705915927887, Label: 662\n",
      "Model name: batch_83, Accuracy: 0.9959726929664612, Label: 678\n",
      "Model name: batch_84, Accuracy: 0.6337589025497437, Label: 70\n",
      "Model name: batch_85, Accuracy: 0.793181300163269, Label: 727\n",
      "Model name: batch_86, Accuracy: 0.9999958276748657, Label: 746\n",
      "Model name: batch_87, Accuracy: 0.7029624581336975, Label: 75\n",
      "Model name: batch_88, Accuracy: 0.9999808073043823, Label: 777\n",
      "Model name: batch_89, Accuracy: 0.9907556176185608, Label: 786\n",
      "Model name: batch_9, Accuracy: 0.9947981834411621, Label: 1152\n",
      "Model name: batch_90, Accuracy: 0.9997041821479797, Label: 804\n",
      "Model name: batch_91, Accuracy: 0.6242943406105042, Label: 83\n",
      "Model name: batch_92, Accuracy: 0.782920777797699, Label: 846\n",
      "Model name: batch_93, Accuracy: 0.7384066581726074, Label: 863\n",
      "Model name: batch_94, Accuracy: 0.6066665053367615, Label: 878\n",
      "Model name: batch_95, Accuracy: 0.991815984249115, Label: 904\n",
      "Model name: batch_96, Accuracy: 0.6032209992408752, Label: 916\n",
      "Model name: batch_97, Accuracy: 0.9510753154754639, Label: 940\n",
      "Model name: batch_98, Accuracy: 0.7171105146408081, Label: 953\n",
      "Model name: batch_99, Accuracy: 0.5871135592460632, Label: 967\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary of best results\n",
    "best_results = {}\n",
    "\n",
    "# iterate tought all models, doing predictions in each models from the image selected\n",
    "# select the label correspondent to the prediction, save it or update it in the correspondent model name.\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    prediction = model.predict(image_array)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    accuracy = prediction[0][predicted_label]\n",
    "    label = label_dict[model_name][predicted_label]\n",
    "    \n",
    "    # Check if model is already present in best results\n",
    "    if model_name not in best_results:\n",
    "        best_results[model_name] = (accuracy, label)\n",
    "    else:\n",
    "        # Update best result if current accuracy is higher\n",
    "        best_accuracy, _ = best_results[model_name]\n",
    "        if accuracy > best_accuracy:\n",
    "            best_results[model_name] = (accuracy, label)\n",
    "\n",
    "# Display the best results\n",
    "for model_name, (accuracy, label) in best_results.items():\n",
    "    print(f\"Model name: {model_name}, Accuracy: {accuracy}, Label: {label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b90709",
   "metadata": {},
   "source": [
    "### Cell 6\n",
    "\n",
    "In this cell we are performing the predictions for each model, we save the best results for each model in a dictionary with the accuracy and the label correspondent to this accuracy, then display all these results at the end with all best predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "907e03e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Name: batch_86\n",
      "Best Accuracy: 0.9999958\n",
      "Label: 746\n",
      "Image Name: 200\n"
     ]
    }
   ],
   "source": [
    "# Select the best accuracy number\n",
    "best_model = max(best_results, key=lambda x: best_results[x][0])\n",
    "best_accuracy, best_label = best_results[best_model]\n",
    "\n",
    "# Display the best accuracy number, label, model name, and image name\n",
    "print(\"Best Model Name:\", best_model)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "print(\"Label:\", best_label)\n",
    "print(\"Image Name:\", image_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3043e5",
   "metadata": {},
   "source": [
    "### Cell 7\n",
    "\n",
    "This cell shows the first findings.\n",
    "The best result accuracy is the models will most likely not be the same as the individual chosen. showing this is not a precision identifyer of a unique individual in 2000 diferent individuals.\n",
    "\n",
    "The output displays the model that has the best accuracy result, how much is this accuracy and the label correspondent to this accuracy.\n",
    "And the image name for the individual tested against all models, so we can know if its correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd949eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: batch_56, Accuracy: 0.9959650039672852, Label: 200\n"
     ]
    }
   ],
   "source": [
    "# Find the model corresponding to the image name\n",
    "\n",
    "model_name_for_image = None\n",
    "\n",
    "for model_name, (accuracy, label) in best_results.items():\n",
    "    if label == image_name:\n",
    "        model_name_for_image = model_name\n",
    "        break\n",
    "\n",
    "# Check if the model exists and display the accuracy\n",
    "if model_name_for_image:\n",
    "    accuracy_for_image, label_for_image = best_results[model_name_for_image]\n",
    "    print(f\"Model name: {model_name_for_image}, Accuracy: {accuracy_for_image}, Label: {label_for_image}\")\n",
    "else:\n",
    "    print(f\"No model found for image: {image_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36360a",
   "metadata": {},
   "source": [
    "### Cell 8\n",
    "\n",
    "In this cell we check if the model that is actually correspondent to the image trained shows in our results at all.\n",
    "If so, It retrieves the model name, its accuracy and the label name and show as an output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ceb858a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum accuracy difference: 0.08328%\n",
      "Second-best accuracy difference: 0.08270%\n",
      "Minimum accuracy difference: 256.64675%\n"
     ]
    }
   ],
   "source": [
    "# Compare the maximum, second-best, and minimum accuracies\n",
    "accuracies = [accuracy for accuracy, _ in best_results.values()]\n",
    "best_accuracy_all_models = max(accuracies)\n",
    "second_best_accuracy_all_models = sorted(accuracies, reverse=True)[1]\n",
    "worst_accuracy_all_models = min(accuracies)\n",
    "\n",
    "# Calculate the percentage differences\n",
    "accuracy_for_image = best_results.get(model_name_for_image)\n",
    "if accuracy_for_image:\n",
    "    max_difference_percentage = (best_accuracy_all_models - accuracy_for_image[0]) / best_accuracy_all_models * 100\n",
    "    second_max_difference_percentage = (second_best_accuracy_all_models - accuracy_for_image[0]) / second_best_accuracy_all_models * 100\n",
    "    min_difference_percentage = (accuracy_for_image[0] - worst_accuracy_all_models) / worst_accuracy_all_models * 100\n",
    "\n",
    "    print(f\"Maximum accuracy difference: {max_difference_percentage:.5f}%\")\n",
    "    print(f\"Second-best accuracy difference: {second_max_difference_percentage:.5f}%\")\n",
    "    print(f\"Minimum accuracy difference: {min_difference_percentage:.5f}%\")\n",
    "else:\n",
    "    print(f\"No accuracy found for model: {model_name_for_image}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f89bc",
   "metadata": {},
   "source": [
    "### Cell 9\n",
    "\n",
    "In this cell we make a few comparissons between the actual individual values and the first, the second best and worst accuracies calculated.\n",
    "The output display the diference in percentage for all these. \n",
    "\n",
    "Its expected the diference to the best being much smaller than the diference to the \"worst\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66bc9a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results with accuracy >= 0.999163031578064: 8\n"
     ]
    }
   ],
   "source": [
    "# Count the number of results with accuracy equal to or greater than the tested image\n",
    "count_high_accuracy = sum(accuracy >= accuracy_for_image[0] for accuracy in accuracies)\n",
    "print(f\"Number of results with accuracy >= {accuracy_for_image[0]}: {count_high_accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c59ff",
   "metadata": {},
   "source": [
    "### Cell 10\n",
    "\n",
    "More comparissons, here we check with position the result found would be in accuracy.\n",
    "This means the framework filtered this individual to the a precision to be \"this number\" over 2000 diferent individuals, showing this works as a filter, some times even a precision Image filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ce88cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate dialog window\n",
    "dialog = tk.Toplevel(root)\n",
    "dialog.lift()\n",
    "dialog.attributes('-topmost', True)\n",
    "dialog.withdraw()\n",
    "# Open the \"Select the Single model file\" dialog and saves the path to it\n",
    "single_model = filedialog.askopenfilename(parent=dialog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344be86d",
   "metadata": {},
   "source": [
    "### Cell 11\n",
    "\n",
    "Extra comparisons, this cell will ask you to load the file for the model that was trained with 2000 classes and 100 epochs.\n",
    "will open a window to find the file located in the folder \"full class model folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "551da793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 72ms/step\n",
      "Accuracy of results for '38':\n",
      "Result: 0.00054666877258569\n",
      "Result: 0.000546460272744298\n",
      "Result: 0.0005463571287691593\n",
      "Result: 0.0005447774892672896\n",
      "Result: 0.000544664217159152\n",
      "Result: 0.0005443018162623048\n",
      "Result: 0.0005439831875264645\n",
      "Result: 0.0005438109510578215\n",
      "Result: 0.0005432708421722054\n",
      "Result: 0.0005423048860393465\n"
     ]
    }
   ],
   "source": [
    "# save paths and load the model\n",
    "singleModel_path = win32api.GetShortPathName(single_model)\n",
    "single_model = load_model(singleModel_path, compile=False)\n",
    "\n",
    "# Perform prediction on the image using the model\n",
    "predictions = single_model.predict(image_array)\n",
    "\n",
    "# Get the top 10 results\n",
    "top_results = np.argsort(predictions[0])[-10:][::-1]  # Get indices of top 10 predictions\n",
    "\n",
    "# Display the top 10 results\n",
    "print(f\"Accuracy of results for '{image_name}':\")\n",
    "for idx in top_results:\n",
    "    accuracy = predictions[0][idx]\n",
    "    print(f\"Result: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed5c6b",
   "metadata": {},
   "source": [
    "### Cell 12\n",
    "\n",
    "This compares the image with the model that was trained for all 2000 classes.\n",
    "as expected, the accuracy was much smaller and even the diference between the probabilities was not great, showing the fact the models return big accuracy values for many diferent images is consistent also in models trained with all classes too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93110d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
